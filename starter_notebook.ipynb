{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e-SparX Starter Notebook\n",
    "\n",
    "Hey there! This notebook will show you how to build a simple e-SparX pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages (make sure you've installed pandas)\n",
    "import esparx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us assume, we have downloaded a raw data file from https://www.epe.ed.tum.de/emt/startseite/. Let us register the dataset in e-SparX. Please choose all artifact names yourself, as in the current e-SparX implementation, each artifact name can only exist once. Also choose a pipeline name (which also must not exist yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_file_name = \"x\" # pick your name here (as string)\n",
    "my_pipeline_name = \"p\" # pick your pipeline name here (as string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's register the raw files in e-SparX! We have to methods to register data artifacts: `register_dataset_pandas` (for `pd.DataFrame` datasets) and `register_dataset_free` (for all remaining datasets). So we will use `register_dataset_free` for our raw files.\n",
    "\n",
    "We want to create a pipeline where the artifact will belong to. This will happen automatically, whenever we mention the pipeline for the first time during registering an artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esparx.register_dataset_free(\n",
    "    name=raw_data_file_name,\n",
    "    description=\"This is my first dataset artifact. It's actually a mock artifact, so don't get too excited.\",\n",
    "    file_type=\"ZIP\",\n",
    "    source_url=\"https://www.epe.ed.tum.de/emt/startseite/\",\n",
    "    pipeline_name=my_pipeline_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now already be able to find your pipeline [here](http://10.152.14.197:3030/)!\n",
    "\n",
    "Next, assume that you're doing some data processing within this Jupyter Notebook. Your outcome will be two different processed datasets.\n",
    "\n",
    "In e-SparX, we want to register this processing notebook and the two processed datasets. Let's go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please give all artifacts your personal name of choice\n",
    "processing_script_name = \"a\" # pick your notebook name here (as string)\n",
    "processed_dataset_1_name = \"b\" # pick your name here (as string)\n",
    "processed_dataset_2_name = \"c\" # pick your name here (as string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume some processing happens here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register script in e-SparX using 'register_code'\n",
    "esparx.register_code(\n",
    "    name=processing_script_name,\n",
    "    description=\"Some fancy processing happened here (or not really).\",\n",
    "    file_type=\"IPYNB\",\n",
    "    pipeline_name=my_pipeline_name,\n",
    "    source_name=raw_data_file_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each register methods supports to specify a `source_name`. This enables you to specify the connectivity of your artifact. In this case, the raw data is the source artifact for your processing notebook. Go ahead and see how that looks in your pipeline online!\n",
    "\n",
    "Also, did you see that when passing the pipeline name, e-SparX did not create the pipeline again, but rather found it and just connected the artifact to it?\n",
    "\n",
    "In general, you're save running a comman again, e-SparX will recognize what's new and what's not new.\n",
    "\n",
    "Finally, let's address the finished datasets. This time, we will assume these are `pd.DataFrames`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_df_1 = pd.DataFrame({\"A\": [1.2, 2.0, 3.5], \"B\": [4.1, 5.0, 6.1]})\n",
    "mock_df_2 = pd.DataFrame({\"C\": [7.3, 8.3, 9.3], \"D\": [10.2, 11.2, 12.2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esparx.register_dataset_pandas(\n",
    "    name=processed_dataset_1_name,\n",
    "    description=\"This is my first processed dataset artifact.\",\n",
    "    file_type=\"none\",\n",
    "    df=mock_df_1, # pass the df here\n",
    "    pipeline_name=my_pipeline_name,\n",
    "    source_name=processing_script_name,\n",
    ")\n",
    "esparx.register_dataset_pandas(\n",
    "    name=processed_dataset_2_name,\n",
    "    description=\"This is my second processed dataset artifact.\",\n",
    "    file_type=\"none\",\n",
    "    df=mock_df_2, # pass the df here\n",
    "    pipeline_name=my_pipeline_name,\n",
    "    source_name=processing_script_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for `register_dataset_pandas`, we actually pass the `pd.DataFrame`. Go to the web and click on the artifact! You will see that e-SparX stored and displays some key information automatically for `pd.DataFrames`.\n",
    "\n",
    "Finally, let's clean up our mock example. You can only delete artifacts which you created yourself. You can only delete pipelines, when they are entirely empty and you created them in the first place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esparx.delete_artifact(raw_data_file_name)\n",
    "esparx.delete_artifact(processing_script_name)\n",
    "esparx.delete_artifact(processed_dataset_1_name)\n",
    "esparx.delete_artifact(processed_dataset_2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esparx.delete_pipeline(my_pipeline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Now you're good to go to grap some last information from the `README` and explore some full-blown usecases in the `usecases` folder. We're excited to see your first ML pipelines soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edl-usecases",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
